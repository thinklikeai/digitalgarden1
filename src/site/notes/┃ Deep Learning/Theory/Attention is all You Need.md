---
{"dg-publish":true,"permalink":"/deep-learning/theory/attention-is-all-you-need/","tags":["gardenEntry"]}
---


---

In this article is where the Transformers were born, explaining its architecture, simpler than the previous used models for Natural Language Processing like [[┃ Deep Learning/Theory/Convolutional Neural Networks\|Convolutional Neural Networks]] or [[┃ Deep Learning/Theory/Recurrent Neural Networks\|Recurrent Neural Networks]]. This previous models were like detectives trying to figure out the next word in a sentence step by step. But the Transformer is like a team of detectives who can work in different parts of the sentence at the same time, making it so much faster and better at understanding languages.
The Transformer is also good at handling big batches of data, which means it can learn faster and perform tasks quickly. So, it's like a super-smart translator and can be used in many applications where understanding and generating sequences of data is important.

![[Attention is All you Need.pdf]]
